"""
Durable Functions orchestrator for research→synthesis workflow.
Replaces broken MultiCallArchitecture with sequential LangChain-based execution.
"""

import logging
import uuid
import json
from typing import Dict, List, Any, Optional

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

from .research_models import ResearchOutput, get_research_output_parser
from .http_utils import is_testing_mode
from .prompt_manager import prompt_manager
from common import get_openai_client
from common.utils import clean_json_response


class DurableOrchestrator:
    """Orchestrates sequential research→synthesis workflow using LangChain and Jinja2."""

    def __init__(self, agent_config):
        """Initialize orchestrator with agent configuration.

        Args:
            agent_config: FullAgentConfig instance for dynamic configuration
        """
        self.agent_config = agent_config
        self._openai_client = None
        self._langchain_client = None

    @property
    def openai_client(self):
        """Lazy-initialized OpenAI client."""
        if self._openai_client is None:
            self._openai_client = get_openai_client()
        return self._openai_client

    @property
    def langchain_client(self):
        """Lazy-initialized LangChain ChatOpenAI client."""
        if self._langchain_client is None:
            self._langchain_client = ChatOpenAI(
                model=self.agent_config.get_model('research'),
            )
        return self._langchain_client

    def create_research_plan(
        self, user_input: Dict[str, Any], budget_tier: str
    ) -> Dict[str, Any]:
        """Create research plan based on budget tier allocation.

        Args:
            user_input: User's input data
            budget_tier: Selected budget tier (basic/standard/premium)

        Returns:
            Research plan with topics and call allocation
        """
        # Find tier configuration
        tier_config = None
        for tier in self.agent_config.get_budget_tiers():
            if tier.name == budget_tier:
                tier_config = tier
                break

        if tier_config is None:
            available_tiers = [t.name for t in self.agent_config.get_budget_tiers()]
            raise ValueError(
                f"Invalid budget tier '{budget_tier}'. Available: {available_tiers}"
            )

        # Calculate research vs synthesis allocation
        total_calls = tier_config.calls
        synthesis_calls = 1  # Always 1 synthesis call
        research_calls = max(0, total_calls - synthesis_calls)

        # Generate research topics based on user input
        research_topics = self._generate_research_topics(user_input, research_calls)

        return {
            "tier": budget_tier,
            "total_calls": total_calls,
            "research_calls": research_calls,
            "synthesis_calls": synthesis_calls,
            "research_topics": research_topics,
            "user_input": user_input,
        }

    def _generate_research_topics(
        self, user_input: Dict[str, Any], num_topics: int
    ) -> List[str]:
        """Generate research topics using OpenAI planning agent.

        Args:
            user_input: User's input data
            num_topics: Number of research topics to generate

        Returns:
            List of research topics generated by planning agent
        """
        if num_topics == 0:
            return []

        if is_testing_mode():
            # Return mock topics for testing - use actual user input fields without fallbacks
            # If required fields are missing, let it fail explicitly
            idea_overview = user_input.get('Idea_Overview', 'MISSING_IDEA_OVERVIEW')
            deliverable = user_input.get('Deliverable', 'MISSING_DELIVERABLE')

            base_topics = [
                f"Market analysis for {idea_overview}",
                f"Competitive landscape for {deliverable}",
                f"Technical feasibility analysis",
                f"Business model evaluation",
            ]
            return base_topics[:num_topics]

        # Get agent personality to understand what type of analysis this is
        agent_personality = self.agent_config.definition.starter_prompt

        # Use centralized research planning template from platform.yaml
        planning_prompt = prompt_manager.format_research_planning_prompt(
            agent_personality=agent_personality,
            user_input=user_input,
            num_topics=num_topics,
        )

        try:
            # Use OpenAI to generate research plan
            response = self.openai_client.responses.create(
                model=self.agent_config.get_model('planning'),
                input=[
                    {
                        "role": "user",
                        "content": [{"type": "input_text", "text": planning_prompt}],
                    }
                ],
                background=False,  # Synchronous for planning
                reasoning={"summary": "auto"},
            )

            # Parse response
            if hasattr(response, 'output') and response.output:
                response_text = response.output[-1].content[0].text.strip()

                # Clean JSON response using centralized utility
                cleaned_response = clean_json_response(response_text)

                # Parse JSON array
                topics = json.loads(cleaned_response)

                if isinstance(topics, list) and len(topics) == num_topics:
                    logging.info(
                        f"Generated {len(topics)} research topics using planning agent"
                    )
                    return topics
                else:
                    logging.warning(
                        f"Planning agent returned {len(topics)} topics, expected {num_topics}"
                    )
                    # Adjust list size if needed
                    if len(topics) > num_topics:
                        return topics[:num_topics]
                    else:
                        # Pad with generic topics if needed
                        while len(topics) < num_topics:
                            topics.append(
                                f"Additional analysis aspect {len(topics) + 1}"
                            )
                        return topics
            else:
                raise ValueError("No output from planning agent")

        except Exception as e:
            logging.error(f"Research planning failed: {str(e)}")
            raise RuntimeError(f"Failed to generate research topics: {str(e)}")

    async def execute_research_call(
        self, research_topic: str, user_input: Dict[str, Any]
    ) -> ResearchOutput:
        """Execute single research call using universal analysis_call template + LangChain.

        Args:
            research_topic: Research topic to investigate
            user_input: Original user input for context

        Returns:
            Structured ResearchOutput from LangChain parsing
        """
        if is_testing_mode():
            # Return mock ResearchOutput for testing
            return ResearchOutput(
                research_topic=research_topic,
                summary=f"Mock research summary for {research_topic}",
                key_findings=[
                    f"Mock finding 1 for {research_topic}",
                    f"Mock finding 2 for {research_topic}",
                ],
                sources_consulted=["mock search", "test data"],
                confidence_level="medium",
            )

        # Use centralized research_call template from platform.yaml
        research_prompt = prompt_manager.format_research_call_prompt(
            starter_prompt=self.agent_config.definition.starter_prompt,
            research_topic=research_topic,
            user_input=user_input,
            json_format_instructions=get_research_output_parser().get_format_instructions(),
        )

        try:
            # Execute LangChain call
            message = HumanMessage(content=research_prompt)
            response = await self.langchain_client.ainvoke([message])

            # Parse structured output
            parser = get_research_output_parser()
            parsed_result = parser.parse(response.content)

            logging.info(f"Completed research call for: {research_topic}")
            return parsed_result

        except Exception as e:
            logging.error(f"Research call failed for {research_topic}: {str(e)}")
            # Don't mask failures with fake data - raise the error to fail fast
            raise RuntimeError(
                f"Research call failed for '{research_topic}': {str(e)}. This indicates a system issue that needs immediate attention."
            )

    async def execute_synthesis_call(
        self, research_results: List[ResearchOutput], user_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute synthesis call combining ALL research results using Jinja2 templates.

        Args:
            research_results: List of ALL ResearchOutput objects from research phase
            user_input: Original user input data

        Returns:
            Final analysis result dictionary
        """
        if is_testing_mode():
            return {
                "Analysis_Result": "Mock synthesis combining all research results",
                "Overall_Rating": "8/10",
                "research_sources": len(research_results),
            }

        # Use existing universal synthesis_call template from platform.yaml via prompt_manager
        synthesis_prompt = prompt_manager.format_synthesis_call_prompt(
            research_results=research_results,
            user_input=user_input,
            agent_personality=self.agent_config.definition.starter_prompt,
            output_fields=self.agent_config.schema.output_fields,
        )

        try:
            # Use existing OpenAI client for synthesis (matches current system)
            response = self.openai_client.responses.create(
                model=self.agent_config.get_model('synthesis'),
                input=[
                    {
                        "role": "user",
                        "content": [{"type": "input_text", "text": synthesis_prompt}],
                    }
                ],
                background=False,  # Synchronous for final result
                tools=[{"type": "web_search_preview"}],
                reasoning={"summary": "auto"},
            )

            # Extract response text
            if hasattr(response, 'output') and response.output:
                response_text = response.output[-1].content[0].text

                # Try to parse JSON response using centralized utility
                try:
                    cleaned_response = clean_json_response(response_text)
                    result = json.loads(cleaned_response)
                    result["synthesis_sources"] = len(research_results)
                    return result
                except json.JSONDecodeError:
                    # Fallback if not JSON
                    return {
                        "Analysis_Result": response_text,
                        "synthesis_sources": len(research_results),
                    }
            else:
                raise ValueError("No output from synthesis call")

        except Exception as e:
            logging.error(f"Synthesis call failed: {str(e)}")
            return {
                "Analysis_Result": f"Synthesis failed: {str(e)}",
                "synthesis_sources": len(research_results),
                "error": True,
            }

    def create_initial_workflow_response(
        self, user_input: Dict[str, Any], budget_tier: str
    ) -> Dict[str, Any]:
        """Create initial workflow response with research plan for fast return.

        Args:
            user_input: User's input data
            budget_tier: Selected budget tier

        Returns:
            Initial workflow response with job ID and research plan
        """
        try:
            # Create research plan (always done first)
            research_plan = self.create_research_plan(user_input, budget_tier)
            
            # Generate job ID for tracking
            job_id = f"durable_{uuid.uuid4().hex}"

            return {
                "job_id": job_id,
                "status": "processing",
                "research_calls_made": 0,
                "synthesis_calls_made": 0,
                "research_plan": research_plan,
                "final_result": None
            }

        except Exception as e:
            logging.error(f"Initial workflow creation failed: {str(e)}")
            job_id = f"failed_{uuid.uuid4().hex}"
            return {
                "job_id": job_id,
                "status": "failed",
                "error": str(e),
                "research_calls_made": 0,
                "synthesis_calls_made": 0,
                "research_plan": None
            }

    async def complete_remaining_workflow(
        self, job_id: str, research_plan: Dict[str, Any], user_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Complete the remaining workflow (research + synthesis) for a job.
        
        This method executes the full research→synthesis workflow that was
        started by create_initial_workflow_response().
        
        Args:
            job_id: Job identifier for tracking
            research_plan: Research plan created during fast return
            user_input: Original user input data
            
        Returns:
            Complete workflow result with final analysis
        """
        logging.info(f"[DURABLE-WORKFLOW] Starting remaining workflow for job: {job_id}")
        logging.info(f"[DURABLE-WORKFLOW] Research plan: {research_plan}")
        
        try:
            # Extract research topics from the plan
            research_topics = research_plan.get("research_topics", [])
            logging.info(f"[DURABLE-WORKFLOW] Found {len(research_topics)} research topics")
            
            # Execute all research calls sequentially
            research_results = []
            for i, topic in enumerate(research_topics):
                logging.info(f"[DURABLE-WORKFLOW] Executing research call {i+1}/{len(research_topics)}: {topic}")
                try:
                    research_result = await self.execute_research_call(topic, user_input)
                    research_results.append(research_result)
                    logging.info(f"[DURABLE-WORKFLOW] Completed research call {i+1}: {research_result.research_topic}")
                except Exception as e:
                    logging.error(f"[DURABLE-WORKFLOW] Research call {i+1} failed: {str(e)}")
                    # Continue with other research calls rather than failing completely
                    continue
            
            logging.info(f"[DURABLE-WORKFLOW] Completed {len(research_results)} research calls successfully")
            
            # Execute synthesis call with all research results (even if empty for basic tier)
            logging.info(f"[DURABLE-WORKFLOW] Starting synthesis with {len(research_results)} research results")
            
            if len(research_results) == 0:
                logging.info(f"[DURABLE-WORKFLOW] Basic tier - running synthesis with user input only (no research)")
            
            final_result = await self.execute_synthesis_call(research_results, user_input)
            logging.info(f"[DURABLE-WORKFLOW] Synthesis completed successfully")
            
            return {
                "status": "completed",
                "job_id": job_id,
                "research_calls_made": len(research_results),
                "synthesis_calls_made": 1,
                "final_result": final_result,
                "research_plan": research_plan
            }
                
        except Exception as e:
            logging.error(f"[DURABLE-WORKFLOW] Complete workflow failed for job {job_id}: {str(e)}")
            return {
                "status": "failed",
                "job_id": job_id,
                "error": str(e),
                "research_calls_made": len(research_results) if 'research_results' in locals() else 0,
                "synthesis_calls_made": 0,
                "final_result": None
            }
