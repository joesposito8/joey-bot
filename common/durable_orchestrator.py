"""
Durable Functions orchestrator for async job polling workflow.
Contains only the methods needed for async job management with OpenAI Deep Research API.
"""

import logging
import uuid
from typing import Dict, List, Any, Optional

from .research_models import ResearchOutput, get_research_output_parser, get_json_list_parser, get_json_dict_parser
from .http_utils import is_testing_mode
from .prompt_manager import prompt_manager
from common import get_openai_client


class DurableOrchestrator:
    """Orchestrates async job polling workflow using OpenAI Deep Research API."""

    def __init__(self, agent_config):
        """Initialize orchestrator with agent configuration.

        Args:
            agent_config: FullAgentConfig instance for dynamic configuration
        """
        self.agent_config = agent_config
        self._openai_client = None

    @property
    def openai_client(self):
        """Lazy-initialized OpenAI client."""
        if self._openai_client is None:
            self._openai_client = get_openai_client()
        return self._openai_client

    def create_research_plan(
        self, user_input: Dict[str, Any], budget_tier: str
    ) -> Dict[str, Any]:
        """Create research plan based on budget tier allocation.

        Args:
            user_input: User's input data
            budget_tier: Selected budget tier (basic/standard/premium)

        Returns:
            Research plan with topics and call allocation
        """
        # Find tier configuration
        tier_config = None
        for tier in self.agent_config.get_budget_tiers():
            if tier.name == budget_tier:
                tier_config = tier
                break

        if tier_config is None:
            available_tiers = [t.name for t in self.agent_config.get_budget_tiers()]
            raise ValueError(
                f"Invalid budget tier '{budget_tier}'. Available: {available_tiers}"
            )

        # Calculate research vs synthesis allocation
        total_calls = tier_config.calls
        synthesis_calls = 1  # Always 1 synthesis call
        research_calls = max(0, total_calls - synthesis_calls)

        # Generate research topics based on user input
        research_topics = self._generate_research_topics(user_input, research_calls)

        return {
            "tier": budget_tier,
            "total_calls": total_calls,
            "research_calls": research_calls,
            "synthesis_calls": synthesis_calls,
            "research_topics": research_topics,
            "user_input": user_input,
        }

    def _generate_research_topics(
        self, user_input: Dict[str, Any], num_topics: int
    ) -> List[str]:
        """Generate research topics using OpenAI planning agent.

        Args:
            user_input: User's input data
            num_topics: Number of research topics to generate

        Returns:
            List of research topics generated by planning agent
        """
        if num_topics == 0:
            return []

        if is_testing_mode():
            # Return mock topics for testing - use actual user input fields without fallbacks
            # If required fields are missing, let it fail explicitly
            idea_overview = user_input.get('Idea_Overview', 'MISSING_IDEA_OVERVIEW')
            deliverable = user_input.get('Deliverable', 'MISSING_DELIVERABLE')

            base_topics = [
                f"Market analysis for {idea_overview}",
                f"Competitive landscape for {deliverable}",
                f"Technical feasibility analysis",
                f"Business model evaluation",
            ]
            return base_topics[:num_topics]

        # Get agent personality to understand what type of analysis this is
        agent_personality = self.agent_config.definition.starter_prompt

        # Use centralized research planning template from platform.yaml
        planning_prompt = prompt_manager.format_research_planning_prompt(
            agent_personality=agent_personality,
            user_input=user_input,
            num_topics=num_topics,
        )

        try:
            # Use OpenAI to generate research plan
            response = self.openai_client.responses.create(
                model=self.agent_config.get_model('planning'),
                input=[
                    {
                        "role": "user",
                        "content": [{"type": "input_text", "text": planning_prompt}],
                    }
                ],
                background=False,  # Synchronous for planning
                reasoning={"summary": "auto"},
            )

            # Parse response with automatic JSON fixing
            if hasattr(response, 'output') and response.output:
                response_text = response.output[-1].content[0].text.strip()

                # Use robust JSON parser with automatic error correction
                parser = get_json_list_parser()
                topics = parser.parse(response_text)

                if isinstance(topics, list) and len(topics) == num_topics:
                    logging.info(
                        f"Generated {len(topics)} research topics using planning agent"
                    )
                    return topics
                else:
                    logging.warning(
                        f"Planning agent returned {len(topics)} topics, expected {num_topics}"
                    )
                    # Adjust list size if needed
                    if len(topics) > num_topics:
                        return topics[:num_topics]
                    else:
                        # Pad with generic topics if needed
                        while len(topics) < num_topics:
                            topics.append(
                                f"Additional analysis aspect {len(topics) + 1}"
                            )
                        return topics
            else:
                raise ValueError("No output from planning agent")

        except Exception as e:
            logging.error(f"Research planning failed: {str(e)}")
            raise RuntimeError(f"Failed to generate research topics: {str(e)}")

    async def start_research_job(
        self, research_topic: str, user_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Start an async research job using OpenAI Deep Research API.
        
        Args:
            research_topic: Research topic to investigate
            user_input: Original user input for context
            
        Returns:
            Dict with job_id and status for polling
        """
        if is_testing_mode():
            import uuid
            return {
                "job_id": f"test_research_{uuid.uuid4().hex[:8]}",
                "status": "started"
            }
        
        # Use centralized research_call template from platform.yaml
        research_prompt = prompt_manager.format_research_call_prompt(
            starter_prompt=self.agent_config.definition.starter_prompt,
            research_topic=research_topic,
            user_input=user_input,
            json_format_instructions=get_research_output_parser().get_format_instructions(),
        )
        
        try:
            # Start async research job with OpenAI Deep Research API
            response = self.openai_client.responses.create(
                model=self.agent_config.get_model('research'),
                input=[
                    {
                        "role": "user",
                        "content": [{"type": "input_text", "text": research_prompt}],
                    }
                ],
                background=True,  # Async job - runs on OpenAI's servers
                reasoning={"summary": "auto"},
            )
            
            # Extract job ID from response
            job_id = response.id  # OpenAI returns job ID for background tasks
            logging.info(f"Started async research job {job_id} for topic: {research_topic}")
            
            return {
                "job_id": job_id,
                "status": "started",
                "research_topic": research_topic
            }
            
        except Exception as e:
            logging.error(f"Failed to start research job for {research_topic}: {str(e)}")
            raise RuntimeError(f"Failed to start research job for '{research_topic}': {str(e)}")

    async def check_job_status(self, job_id: str) -> Dict[str, Any]:
        """Check the status of an async job using OpenAI Deep Research API.
        
        Args:
            job_id: Job ID to check
            
        Returns:
            Dict with job_id, status, and ready_for_fetch flag
        """
        if is_testing_mode():
            import random
            if random.random() < 0.7:  # 70% chance job is done
                return {
                    "job_id": job_id,
                    "status": "succeeded",
                    "ready_for_fetch": True
                }
            else:
                return {
                    "job_id": job_id,
                    "status": "running",
                    "ready_for_fetch": False
                }
        
        try:
            # Check job status using OpenAI API
            status_response = self.openai_client.responses.retrieve(job_id)
            
            status = status_response.status  # 'running', 'succeeded', 'failed'
            ready_for_fetch = status == 'succeeded'
            
            logging.info(f"Job {job_id} status: {status}")
            
            return {
                "job_id": job_id,
                "status": status,
                "ready_for_fetch": ready_for_fetch
            }
            
        except Exception as e:
            logging.error(f"Failed to check status for job {job_id}: {str(e)}")
            return {
                "job_id": job_id,
                "status": "failed",
                "ready_for_fetch": False,
                "error": str(e)
            }

    async def fetch_research_result(self, job_id: str, research_topic: str) -> ResearchOutput:
        """Fetch the result of a completed research job.
        
        Args:
            job_id: Job ID to fetch
            research_topic: Original research topic for context
            
        Returns:
            Structured ResearchOutput from completed job
        """
        if is_testing_mode():
            return ResearchOutput(
                research_topic=research_topic,
                summary=f"Mock research summary for {research_topic}",
                key_findings=[
                    f"Mock finding 1 for {research_topic}",
                    f"Mock finding 2 for {research_topic}",
                ],
                sources_consulted=["mock search", "test data"],
                confidence_level="medium",
            )
        
        try:
            # Fetch completed job result
            result_response = self.openai_client.responses.retrieve(job_id)
            
            if not hasattr(result_response, 'output') or not result_response.output:
                raise ValueError(f"No output available for job {job_id}")
            
            response_text = result_response.output[-1].content[0].text
            
            # Parse structured output with automatic JSON fixing
            parser = get_research_output_parser()
            parsed_result = parser.parse(response_text)
            
            logging.info(f"Successfully fetched research result for job: {job_id}")
            return parsed_result
            
        except Exception as e:
            logging.error(f"Failed to fetch result for job {job_id}: {str(e)}")
            raise RuntimeError(f"Failed to fetch research result for job '{job_id}': {str(e)}")

    async def start_synthesis_job(
        self, research_results: List[ResearchOutput], user_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Start an async synthesis job using OpenAI Deep Research API.
        
        Args:
            research_results: List of completed research results
            user_input: Original user input data
            
        Returns:
            Dict with job_id and status for polling
        """
        if is_testing_mode():
            import uuid
            return {
                "job_id": f"test_synthesis_{uuid.uuid4().hex[:8]}",
                "status": "started"
            }
        
        # Use existing universal synthesis_call template
        synthesis_prompt = prompt_manager.format_synthesis_call_prompt(
            research_results=research_results,
            user_input=user_input,
            agent_personality=self.agent_config.definition.starter_prompt,
            output_fields=self.agent_config.schema.output_fields,
        )
        
        try:
            # Start async synthesis job
            response = self.openai_client.responses.create(
                model=self.agent_config.get_model('synthesis'),
                input=[
                    {
                        "role": "user",
                        "content": [{"type": "input_text", "text": synthesis_prompt}],
                    }
                ],
                background=True,  # Async job - runs on OpenAI's servers
                tools=[{"type": "web_search_preview"}],
                reasoning={"summary": "auto"},
            )
            
            job_id = response.id
            logging.info(f"Started async synthesis job: {job_id}")
            
            return {
                "job_id": job_id,
                "status": "started"
            }
            
        except Exception as e:
            logging.error(f"Failed to start synthesis job: {str(e)}")
            raise RuntimeError(f"Failed to start synthesis job: {str(e)}")

    async def fetch_synthesis_result(self, job_id: str) -> Dict[str, Any]:
        """Fetch the result of a completed synthesis job.
        
        Args:
            job_id: Job ID to fetch
            
        Returns:
            Final analysis result dictionary
        """
        if is_testing_mode():
            return {
                "Analysis_Result": "Mock synthesis combining all research results",
                "Overall_Rating": "8/10",
                "synthesis_sources": 0,
            }
        
        try:
            # Fetch completed synthesis result
            result_response = self.openai_client.responses.retrieve(job_id)
            
            if not hasattr(result_response, 'output') or not result_response.output:
                raise ValueError(f"No synthesis output available for job {job_id}")
            
            response_text = result_response.output[-1].content[0].text
            
            # Use robust JSON parser with automatic error correction
            try:
                parser = get_json_dict_parser()
                result = parser.parse(response_text)
                logging.info(f"Successfully fetched synthesis result for job: {job_id}")
                return result
            except Exception:
                # Fallback if parsing completely fails
                logging.warning(f"Failed to parse synthesis result for job {job_id}, using fallback")
                return {
                    "Analysis_Result": response_text,
                    "synthesis_sources": 0,
                }
                
        except Exception as e:
            logging.error(f"Failed to fetch synthesis result for job {job_id}: {str(e)}")
            raise RuntimeError(f"Failed to fetch synthesis result for job '{job_id}': {str(e)}")

    def create_initial_workflow_response(
        self, user_input: Dict[str, Any], budget_tier: str, job_id: str = None
    ) -> Dict[str, Any]:
        """Create initial workflow response with research plan for fast return.

        Args:
            user_input: User's input data
            budget_tier: Selected budget tier
            job_id: Optional deterministic job ID for deduplication

        Returns:
            Initial workflow response with job ID and research plan
        """
        try:
            # Create research plan (always done first)
            research_plan = self.create_research_plan(user_input, budget_tier)
            
            # Use provided job ID or generate UUID fallback
            if job_id is None:
                job_id = f"durable_{uuid.uuid4().hex}"

            return {
                "job_id": job_id,
                "status": "processing",
                "research_calls_made": 0,
                "synthesis_calls_made": 0,
                "research_plan": research_plan,
                "final_result": None
            }

        except Exception as e:
            logging.error(f"Initial workflow creation failed: {str(e)}")
            job_id = f"failed_{uuid.uuid4().hex}"
            return {
                "job_id": job_id,
                "status": "failed",
                "error": str(e),
                "research_calls_made": 0,
                "synthesis_calls_made": 0,
                "research_plan": None
            }