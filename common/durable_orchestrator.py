"""
Durable Functions orchestrator for research→synthesis workflow.
Replaces broken MultiCallArchitecture with sequential LangChain-based execution.
"""

import logging
import uuid
import json
from typing import Dict, List, Any, Optional

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

from .research_models import ResearchOutput, get_research_output_parser
from .http_utils import is_testing_mode
from .prompt_manager import prompt_manager
from common import get_openai_client


class DurableOrchestrator:
    """Orchestrates sequential research→synthesis workflow using LangChain and Jinja2."""
    
    def __init__(self, agent_config):
        """Initialize orchestrator with agent configuration.
        
        Args:
            agent_config: FullAgentConfig instance for dynamic configuration
        """
        self.agent_config = agent_config
        self._openai_client = None
        self._langchain_client = None
    
    @property
    def openai_client(self):
        """Lazy-initialized OpenAI client."""
        if self._openai_client is None:
            self._openai_client = get_openai_client()
        return self._openai_client
    
    @property
    def langchain_client(self):
        """Lazy-initialized LangChain ChatOpenAI client."""
        if self._langchain_client is None:
            self._langchain_client = ChatOpenAI(
                model=self.agent_config.get_model('analysis'),
                temperature=0.1
            )
        return self._langchain_client
    
    def create_research_plan(self, user_input: Dict[str, Any], budget_tier: str) -> Dict[str, Any]:
        """Create research plan based on budget tier allocation.
        
        Args:
            user_input: User's input data
            budget_tier: Selected budget tier (basic/standard/premium)
            
        Returns:
            Research plan with topics and call allocation
        """
        # Find tier configuration
        tier_config = None
        for tier in self.agent_config.get_budget_tiers():
            if tier.name == budget_tier:
                tier_config = tier
                break
        
        if tier_config is None:
            available_tiers = [t.name for t in self.agent_config.get_budget_tiers()]
            raise ValueError(f"Invalid budget tier '{budget_tier}'. Available: {available_tiers}")
        
        # Calculate research vs synthesis allocation
        total_calls = tier_config.calls
        synthesis_calls = 1  # Always 1 synthesis call
        research_calls = max(0, total_calls - synthesis_calls)
        
        # Generate research topics based on user input
        research_topics = self._generate_research_topics(user_input, research_calls)
        
        return {
            "tier": budget_tier,
            "total_calls": total_calls,
            "research_calls": research_calls,
            "synthesis_calls": synthesis_calls,
            "research_topics": research_topics,
            "user_input": user_input
        }
    
    def _generate_research_topics(self, user_input: Dict[str, Any], num_topics: int) -> List[str]:
        """Generate research topics using OpenAI planning agent.
        
        Args:
            user_input: User's input data
            num_topics: Number of research topics to generate
            
        Returns:
            List of research topics generated by planning agent
        """
        if num_topics == 0:
            return []
        
        if is_testing_mode():
            # Return mock topics for testing
            base_topics = [
                f"Market analysis for {user_input.get('Idea_Overview', 'idea')}",
                f"Competitive landscape for {user_input.get('Deliverable', 'product')}",
                f"Technical feasibility analysis",
                f"Business model evaluation"
            ]
            return base_topics[:num_topics]
        
        # Get agent personality to understand what type of analysis this is
        agent_personality = self.agent_config.definition.starter_prompt
        
        # Use centralized research planning template from platform.yaml
        planning_prompt = prompt_manager.format_research_planning_prompt(
            agent_personality=agent_personality,
            user_input=user_input,
            num_topics=num_topics
        )

        try:
            # Use OpenAI to generate research plan
            response = self.openai_client.responses.create(
                model=self.agent_config.get_model('analysis'),
                input=[{"role": "user", "content": [{"type": "input_text", "text": planning_prompt}]}],
                background=False,  # Synchronous for planning
                reasoning={"summary": "auto"}
            )
            
            # Parse response
            if hasattr(response, 'output') and response.output:
                response_text = response.output[-1].content[0].text.strip()
                
                # Clean JSON response
                if response_text.startswith('```json'):
                    response_text = response_text[7:]
                if response_text.startswith('```'):
                    response_text = response_text[3:]
                if response_text.endswith('```'):
                    response_text = response_text[:-3]
                response_text = response_text.strip()
                
                # Parse JSON array
                topics = json.loads(response_text)
                
                if isinstance(topics, list) and len(topics) == num_topics:
                    logging.info(f"Generated {len(topics)} research topics using planning agent")
                    return topics
                else:
                    logging.warning(f"Planning agent returned {len(topics)} topics, expected {num_topics}")
                    # Adjust list size if needed
                    if len(topics) > num_topics:
                        return topics[:num_topics]
                    else:
                        # Pad with generic topics if needed
                        while len(topics) < num_topics:
                            topics.append(f"Additional analysis aspect {len(topics) + 1}")
                        return topics
            else:
                raise ValueError("No output from planning agent")
                
        except Exception as e:
            logging.error(f"Research planning failed: {str(e)}")
            raise RuntimeError(f"Failed to generate research topics: {str(e)}")
    
    def execute_research_call(self, research_topic: str, user_input: Dict[str, Any]) -> ResearchOutput:
        """Execute single research call using universal analysis_call template + LangChain.
        
        Args:
            research_topic: Research topic to investigate
            user_input: Original user input for context
            
        Returns:
            Structured ResearchOutput from LangChain parsing
        """
        if is_testing_mode():
            # Return mock ResearchOutput for testing
            return ResearchOutput(
                research_topic=research_topic,
                summary=f"Mock research summary for {research_topic}",
                key_findings=[
                    f"Mock finding 1 for {research_topic}",
                    f"Mock finding 2 for {research_topic}"
                ],
                sources_consulted=["mock search", "test data"],
                confidence_level="medium"
            )
        
        # Use centralized research_call template from platform.yaml
        research_prompt = prompt_manager.format_research_call_prompt(
            starter_prompt=self.agent_config.definition.starter_prompt,
            research_topic=research_topic,
            user_input=user_input,
            json_format_instructions=get_research_output_parser().get_format_instructions()
        )
        
        try:
            # Execute LangChain call
            message = HumanMessage(content=research_prompt)
            response = self.langchain_client.invoke([message])
            
            # Parse structured output
            parser = get_research_output_parser()
            parsed_result = parser.parse(response.content)
            
            logging.info(f"Completed research call for: {research_topic}")
            return parsed_result
            
        except Exception as e:
            logging.error(f"Research call failed for {research_topic}: {str(e)}")
            # Return fallback result
            return ResearchOutput(
                research_topic=research_topic,
                summary=f"Research failed: {str(e)}",
                key_findings=["Unable to complete research due to error"],
                confidence_level="low"
            )
    
    def execute_synthesis_call(self, research_results: List[ResearchOutput], user_input: Dict[str, Any]) -> Dict[str, Any]:
        """Execute synthesis call combining ALL research results using Jinja2 templates.
        
        Args:
            research_results: List of ALL ResearchOutput objects from research phase
            user_input: Original user input data
            
        Returns:
            Final analysis result dictionary
        """
        if is_testing_mode():
            return {
                "Analysis_Result": "Mock synthesis combining all research results",
                "Overall_Rating": "8/10",
                "research_sources": len(research_results)
            }
        
        # Use existing universal synthesis_call template from platform.yaml via prompt_manager
        synthesis_prompt = prompt_manager.format_synthesis_call_prompt(
            research_results=research_results,
            user_input=user_input,
            agent_personality=self.agent_config.definition.starter_prompt,
            output_fields=self.agent_config.schema.output_fields
        )
        
        try:
            # Use existing OpenAI client for synthesis (matches current system)
            response = self.openai_client.responses.create(
                model=self.agent_config.get_model('analysis'),
                input=[{"role": "user", "content": [{"type": "input_text", "text": synthesis_prompt}]}],
                background=False,  # Synchronous for final result
                tools=[{"type": "web_search_preview"}],
                reasoning={"summary": "auto"}
            )
            
            # Extract response text
            if hasattr(response, 'output') and response.output:
                response_text = response.output[-1].content[0].text
                
                # Try to parse JSON response
                try:
                    result = json.loads(response_text)
                    result["synthesis_sources"] = len(research_results)
                    return result
                except json.JSONDecodeError:
                    # Fallback if not JSON
                    return {
                        "Analysis_Result": response_text,
                        "synthesis_sources": len(research_results)
                    }
            else:
                raise ValueError("No output from synthesis call")
                
        except Exception as e:
            logging.error(f"Synthesis call failed: {str(e)}")
            return {
                "Analysis_Result": f"Synthesis failed: {str(e)}",
                "synthesis_sources": len(research_results),
                "error": True
            }
    
    def execute_workflow(self, user_input: Dict[str, Any], budget_tier: str) -> Dict[str, Any]:
        """Execute complete research→synthesis workflow.
        
        Args:
            user_input: User's input data
            budget_tier: Selected budget tier
            
        Returns:
            Workflow execution result with job ID and status
        """
        try:
            # Create research plan
            research_plan = self.create_research_plan(user_input, budget_tier)
            
            # Execute all research calls sequentially (rate-limit friendly)
            research_results = []
            for topic in research_plan["research_topics"]:
                result = self.execute_research_call(topic, user_input)
                research_results.append(result)
                logging.info(f"Completed research: {topic}")
            
            # Execute synthesis with ALL research results
            synthesis_result = self.execute_synthesis_call(research_results, user_input)
            
            # Generate job ID and return result
            job_id = f"durable_{uuid.uuid4()}"
            
            return {
                "job_id": job_id,
                "status": "completed",
                "research_calls_made": len(research_results),
                "synthesis_calls_made": 1,
                "research_plan": research_plan,
                "final_result": synthesis_result
            }
            
        except Exception as e:
            logging.error(f"Workflow execution failed: {str(e)}")
            job_id = f"failed_{uuid.uuid4()}"
            return {
                "job_id": job_id,
                "status": "failed",
                "error": str(e),
                "research_calls_made": 0,
                "synthesis_calls_made": 0
            }